{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code performs classification on the forest cover type dataset using a deep learning model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The code begins by importing the necessary libraries for data processing and model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import  InputLayer\n",
    "from tensorflow.keras.layers import  Dense\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 581012 entries, 0 to 581011\n",
      "Data columns (total 55 columns):\n",
      " #   Column                              Non-Null Count   Dtype\n",
      "---  ------                              --------------   -----\n",
      " 0   Elevation                           581012 non-null  int64\n",
      " 1   Aspect                              581012 non-null  int64\n",
      " 2   Slope                               581012 non-null  int64\n",
      " 3   Horizontal_Distance_To_Hydrology    581012 non-null  int64\n",
      " 4   Vertical_Distance_To_Hydrology      581012 non-null  int64\n",
      " 5   Horizontal_Distance_To_Roadways     581012 non-null  int64\n",
      " 6   Hillshade_9am                       581012 non-null  int64\n",
      " 7   Hillshade_Noon                      581012 non-null  int64\n",
      " 8   Hillshade_3pm                       581012 non-null  int64\n",
      " 9   Horizontal_Distance_To_Fire_Points  581012 non-null  int64\n",
      " 10  Wilderness_Area1                    581012 non-null  int64\n",
      " 11  Wilderness_Area2                    581012 non-null  int64\n",
      " 12  Wilderness_Area3                    581012 non-null  int64\n",
      " 13  Wilderness_Area4                    581012 non-null  int64\n",
      " 14  Soil_Type1                          581012 non-null  int64\n",
      " 15  Soil_Type2                          581012 non-null  int64\n",
      " 16  Soil_Type3                          581012 non-null  int64\n",
      " 17  Soil_Type4                          581012 non-null  int64\n",
      " 18  Soil_Type5                          581012 non-null  int64\n",
      " 19  Soil_Type6                          581012 non-null  int64\n",
      " 20  Soil_Type7                          581012 non-null  int64\n",
      " 21  Soil_Type8                          581012 non-null  int64\n",
      " 22  Soil_Type9                          581012 non-null  int64\n",
      " 23  Soil_Type10                         581012 non-null  int64\n",
      " 24  Soil_Type11                         581012 non-null  int64\n",
      " 25  Soil_Type12                         581012 non-null  int64\n",
      " 26  Soil_Type13                         581012 non-null  int64\n",
      " 27  Soil_Type14                         581012 non-null  int64\n",
      " 28  Soil_Type15                         581012 non-null  int64\n",
      " 29  Soil_Type16                         581012 non-null  int64\n",
      " 30  Soil_Type17                         581012 non-null  int64\n",
      " 31  Soil_Type18                         581012 non-null  int64\n",
      " 32  Soil_Type19                         581012 non-null  int64\n",
      " 33  Soil_Type20                         581012 non-null  int64\n",
      " 34  Soil_Type21                         581012 non-null  int64\n",
      " 35  Soil_Type22                         581012 non-null  int64\n",
      " 36  Soil_Type23                         581012 non-null  int64\n",
      " 37  Soil_Type24                         581012 non-null  int64\n",
      " 38  Soil_Type25                         581012 non-null  int64\n",
      " 39  Soil_Type26                         581012 non-null  int64\n",
      " 40  Soil_Type27                         581012 non-null  int64\n",
      " 41  Soil_Type28                         581012 non-null  int64\n",
      " 42  Soil_Type29                         581012 non-null  int64\n",
      " 43  Soil_Type30                         581012 non-null  int64\n",
      " 44  Soil_Type31                         581012 non-null  int64\n",
      " 45  Soil_Type32                         581012 non-null  int64\n",
      " 46  Soil_Type33                         581012 non-null  int64\n",
      " 47  Soil_Type34                         581012 non-null  int64\n",
      " 48  Soil_Type35                         581012 non-null  int64\n",
      " 49  Soil_Type36                         581012 non-null  int64\n",
      " 50  Soil_Type37                         581012 non-null  int64\n",
      " 51  Soil_Type38                         581012 non-null  int64\n",
      " 52  Soil_Type39                         581012 non-null  int64\n",
      " 53  Soil_Type40                         581012 non-null  int64\n",
      " 54  class                               581012 non-null  int64\n",
      "dtypes: int64(55)\n",
      "memory usage: 243.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# We will use pandas to read the dataset\n",
    "dataset = pd.read_csv(\"cover_data.csv\")\n",
    "\n",
    "# Let's check the shape of the dataset and other info\n",
    "print(dataset.info())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 283301, 1: 211840, 3: 35754, 7: 20510, 6: 17367, 5: 9493, 4: 2747})\n"
     ]
    }
   ],
   "source": [
    "# Features (X)\n",
    "features = dataset.iloc[:, 0:-1]\n",
    "\n",
    "# Labels (y)\n",
    "labels = dataset.iloc[:, -1]\n",
    "\n",
    "print(Counter(labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the dataset is split into training and testing sets using train_test_split() from sklearn. The test_size parameter is set to 0.3, indicating a 70:30 train-test split.\n",
    "\n",
    "To prepare the data for modeling, the numerical features are standardized using StandardScaler() from sklearn. The columns to be standardized are selected based on their data types (float64 and int64) using select_dtypes(). The ColumnTransformer is used to apply standardization only to the numerical columns, while leaving the remaining columns untouched.\n",
    "\n",
    "The training and testing features are then transformed using the ColumnTransformer object, applying the standardization to the numerical features.\n",
    "\n",
    "The labels are encoded using LabelEncoder() to convert them into integer labels. This step is necessary for compatibility with the model.\n",
    "\n",
    "The integer labels are further transformed into binary vectors using to_categorical() from tensorflow.keras.utils. This step converts the labels into a one-hot encoded format, suitable for multi-class classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = 0.3, random_state=42)\n",
    "\n",
    "# Standardize\n",
    "numerical_features = features.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "numerical_columns = numerical_features.columns\n",
    "\n",
    "# Create a ct object\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ct = ColumnTransformer([(\"only numeric\", StandardScaler(), numerical_columns)], remainder='passthrough')\n",
    "\n",
    "# Transform the test data features_test using ct\n",
    "features_train_scaled = ct.fit_transform(features_train)\n",
    "features_test_scaled = ct.transform(features_test)\n",
    "\n",
    "# Convert the labels into integers\n",
    "label_encoder = LabelEncoder()\n",
    "labels_train_encoded = label_encoder.fit_transform(labels_train)\n",
    "labels_test_encoded = label_encoder.transform(labels_test)\n",
    "\n",
    "# Convert the integer labels into binary vectors\n",
    "labels_train = tensorflow.keras.utils.to_categorical(labels_train_encoded)\n",
    "labels_test = tensorflow.keras.utils.to_categorical(labels_test_encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is initialized as a sequential model using Sequential() from tensorflow.keras.models. This type of model allows stacking multiple layers sequentially.\n",
    "\n",
    "The input layer is added to the model using InputLayer() with the shape parameter set to the number of features in the training data.\n",
    "\n",
    "A hidden layer with 32 units and ReLU activation is added to the model using Dense(). This layer introduces non-linearity to the model.\n",
    "\n",
    "Another hidden layer with 16 units and ReLU activation is added to the model, further capturing complex patterns in the data.\n",
    "\n",
    "The output layer with 7 neurons (corresponding to the 7 classes) and softmax activation is added to the model. Softmax activation ensures that the output values represent class probabilities.\n",
    "\n",
    "The model summary is printed using model.summary(), providing a concise overview of the model architecture, the number of parameters, and the shape of the output at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 32)                1760      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 7)                 119       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,407\n",
      "Trainable params: 2,407\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "input = InputLayer(input_shape = (features_train.shape[1], ))\n",
    "model.add(input)\n",
    "\n",
    "# Hidden layer\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "# Add another hidden layer\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "\n",
    "# Output layer with 7 neurons since we have 7 classes\n",
    "model.add(Dense(7, activation=\"softmax\"))\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is compiled using compile() with the categorical cross-entropy loss, accuracy metric, and Adam optimizer. The learning rate for the optimizer is set to 0.001.\n",
    "\n",
    "An early stopping callback is defined using EarlyStopping(). It monitors the loss during training and stops training if no improvement is observed after 3 epochs.\n",
    "\n",
    "The model is trained using fit(), specifying the training features, labels, number of epochs, batch size, verbosity, and the early stopping callback. The training process is displayed with progress information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12710/12710 [==============================] - 16s 1ms/step - loss: 0.5953 - accuracy: 0.7467\n",
      "Epoch 2/20\n",
      "12710/12710 [==============================] - 18s 1ms/step - loss: 0.5108 - accuracy: 0.7804\n",
      "Epoch 3/20\n",
      "12710/12710 [==============================] - 15s 1ms/step - loss: 0.4856 - accuracy: 0.7920\n",
      "Epoch 4/20\n",
      "12710/12710 [==============================] - 15s 1ms/step - loss: 0.4692 - accuracy: 0.8000\n",
      "Epoch 5/20\n",
      "12710/12710 [==============================] - 15s 1ms/step - loss: 0.4596 - accuracy: 0.8045\n",
      "Epoch 6/20\n",
      "12710/12710 [==============================] - 15s 1ms/step - loss: 0.4518 - accuracy: 0.8076\n",
      "Epoch 7/20\n",
      "12710/12710 [==============================] - 16s 1ms/step - loss: 0.4459 - accuracy: 0.8106\n",
      "Epoch 8/20\n",
      "12710/12710 [==============================] - 16s 1ms/step - loss: 0.4415 - accuracy: 0.8131\n",
      "Epoch 9/20\n",
      "12710/12710 [==============================] - 16s 1ms/step - loss: 0.4377 - accuracy: 0.8144\n",
      "Epoch 10/20\n",
      "12710/12710 [==============================] - 17s 1ms/step - loss: 0.4340 - accuracy: 0.8160\n",
      "Epoch 11/20\n",
      "12710/12710 [==============================] - 17s 1ms/step - loss: 0.4315 - accuracy: 0.8173\n",
      "Epoch 12/20\n",
      "12710/12710 [==============================] - 20s 2ms/step - loss: 0.4297 - accuracy: 0.8183\n",
      "Epoch 13/20\n",
      "12710/12710 [==============================] - 19s 2ms/step - loss: 0.4271 - accuracy: 0.8196\n",
      "Epoch 14/20\n",
      "12710/12710 [==============================] - 16s 1ms/step - loss: 0.4251 - accuracy: 0.8199\n",
      "Epoch 15/20\n",
      "12710/12710 [==============================] - 17s 1ms/step - loss: 0.4236 - accuracy: 0.8209\n",
      "Epoch 16/20\n",
      "12710/12710 [==============================] - 18s 1ms/step - loss: 0.4223 - accuracy: 0.8216\n",
      "Epoch 17/20\n",
      "12710/12710 [==============================] - 18s 1ms/step - loss: 0.4205 - accuracy: 0.8223\n",
      "Epoch 18/20\n",
      "12710/12710 [==============================] - 20s 2ms/step - loss: 0.4184 - accuracy: 0.8234\n",
      "Epoch 19/20\n",
      "12710/12710 [==============================] - 21s 2ms/step - loss: 0.4176 - accuracy: 0.8235\n",
      "Epoch 20/20\n",
      "12710/12710 [==============================] - 16s 1ms/step - loss: 0.4157 - accuracy: 0.8240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20dbe964d00>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "opt = Adam(learning_rate = 0.001)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer = opt)\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "# Fit the model with early stopping\n",
    "model.fit(features_train_scaled, labels_train, epochs=20, batch_size=32, verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicts the labels for the testing features using model.predict(), producing predicted class probabilities.\n",
    "\n",
    "The predicted labels and true labels are obtained by finding the index of the maximum probability using np.argmax(). This converts the one-hot encoded vectors back into class labels.\n",
    "\n",
    "Finally, the classification report is printed using classification_report() from sklearn.metrics. This report provides precision, recall, F1-score, and support for each class, allowing for detailed evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.78      0.81     63556\n",
      "           1       0.82      0.90      0.86     85078\n",
      "           2       0.77      0.85      0.81     10638\n",
      "           3       0.82      0.54      0.65       795\n",
      "           4       0.64      0.39      0.48      2941\n",
      "           5       0.62      0.51      0.56      5227\n",
      "           6       0.92      0.70      0.79      6069\n",
      "\n",
      "    accuracy                           0.82    174304\n",
      "   macro avg       0.78      0.67      0.71    174304\n",
      "weighted avg       0.82      0.82      0.82    174304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(features_test_scaled)\n",
    "\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "y_true_labels = np.argmax(labels_test, axis=1)\n",
    "\n",
    "print(classification_report(y_true_labels, y_pred_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
